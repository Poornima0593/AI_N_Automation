# -*- coding: utf-8 -*-
"""Assignment-2-Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TdVF9OJ_yc3bJOIXXKVcplXTygi9DIof
"""

#Step1: Import the Housing data

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split





housing = pd.read_csv('housing.csv')

housing.info()

#Step2: Check for the missing data in the Dataframe
housing.isnull().sum()

#step4:Split the data in training and test set

# below i have used StratifiedShuffleSplit to split the data.
# for the Assignment i am goi g to used this split mechanisum as it is best in case of MultipleSplits and Cross Validation is in picture.

housing['income_group'] = pd.cut(housing['median_income'],bins= [0.,1.5,3,4.5,6,np.inf], labels= [1,2,3,4,5])
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_group"]): strat_train_set = housing.loc[train_index]
strat_test_set = housing.loc[test_index]

housing = strat_train_set.drop(["median_house_value",'income_group'], axis=1) # drop labels for training set
housing_labels = strat_train_set["median_house_value"].copy()

"""#Creating the pipline for the ML"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

housing_num = housing.drop("ocean_proximity", axis=1)

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
         ('std_scaler', StandardScaler()),
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)

housing_num

from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
        ("num", num_pipeline, num_attribs),
        ("cat", OneHotEncoder(), cat_attribs)
    ])

housing_prepared = full_pipeline.fit_transform(housing)

housing_prepared

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error


svm_reg = SVR(kernel="linear")
svm_reg.fit(housing_prepared, housing_labels)
housing_predictions = svm_reg.predict(housing_prepared)
svm_mse = mean_squared_error(housing_labels, housing_predictions)
svm_rmse = np.sqrt(svm_mse)
svm_rmse

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error


def display_scores(scores):
    print("Scores:", scores)
    print("RMSE:", scores.mean())
    print("Standard deviation:", scores.std())



lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)

from sklearn.tree import DecisionTreeRegressor

param_grid_tree = {
    'max_depth': list(range(1, 20)),
    'min_samples_leaf': list(range(1, 20))
}

tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(housing_prepared, housing_labels)


scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                         scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-scores)
display_scores(tree_rmse_scores)

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(housing_prepared, housing_labels)


scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                          scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-scores)
display_scores(tree_rmse_scores)

from sklearn.ensemble import RandomForestRegressor

param_grid = [
    # try 12 (3×4) combinations of hyperparameters
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    # then try 6 (2×3) combinations with bootstrap set as False
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]


forest_reg = RandomForestRegressor(random_state=42)
forest_reg.fit(housing_prepared, housing_labels)


forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
                                scoring="neg_mean_squared_error", cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)
display_scores(forest_rmse_scores)

from sklearn.model_selection import GridSearchCV

param_grid = [
         {'kernel': ['linear'], 'C': [100, 300, 1000, 3000, 5000]},
         {'kernel': ['rbf'], 'C': [100, 300, 1000, 3000, 5000],
          'gamma': [0.3, 1.0, 3.0, 10, 30]},
     ]

svm_reg = SVR()
grid_search = GridSearchCV(svm_reg, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(housing_prepared, housing_labels)

negative_mse = grid_search.best_score_
rmse = np.sqrt(-negative_mse)
rmse

grid_search.best_params_

from sklearn.model_selection import cross_val_score
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error


svm_reg_rbf = SVR(kernel="rbf", C=5000)
svm_reg_rbf.fit(housing_prepared, housing_labels)

svm_rbf_scores = cross_val_score(svm_reg_rbf, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)
svm_rbf_rmse_scores = np.sqrt(-svm_rbf_scores)
display_scores(svm_rbf_rmse_scores)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVR


param_grid = {
    'kernel': ['linear','rbf'],
    'C': [7000, 10000, 20000, 30000, 40000],
    'gamma': [0.1, 0.3, 1.0, 3.0 ],
}

svm_reg_rsv = SVR()
rsv_search = RandomizedSearchCV(svm_reg_rsv, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)
rsv_search.fit(housing_prepared, housing_labels)

negative_mse = rsv_search.best_score_
rmse = np.sqrt(-negative_mse)
rmse

rsv_search.best_params_

from sklearn.model_selection import cross_val_score
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error


svm_reg_rsv = SVR(kernel="rbf", C=20000)
svm_reg_rsv.fit(housing_prepared, housing_labels)

svm_reg_rsv_scores = cross_val_score(svm_reg_rsv, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)
svm_reg_rsv_rmse_scores = np.sqrt(-svm_reg_rsv_scores)
display_scores(svm_reg_rsv_rmse_scores)

import matplotlib.pyplot as plt
import seaborn as sns

# Example data: model names and their RMSE values
models = ['Linear_regression', 'Discussion_Tree', 'RandonmisedForest\nRegressor', 'SVR_rbf']
#rmse_values = [lin_rmse_scores, tree_rmse_scores, forest_rmse_scores, svm_reg_rsv_rmse_scores]
rmse_values = [
    np.mean(lin_rmse_scores),
    np.mean(tree_rmse_scores),
    np.mean(forest_rmse_scores),
    np.mean(svm_reg_rsv_rmse_scores)
]

# Create a bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x=models, y=rmse_values, palette='viridis')

plt.title('RMSE Comparison Across Models')
plt.xlabel('Models')
plt.ylabel('RMSE')
#plt.ylim(0, max(rmse_values) + 1)  # add some space on y-axis
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

